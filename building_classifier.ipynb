{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNtXpbAAX8zm"
      },
      "source": [
        "**Importing Important Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-S31BqfXX-eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snw7bXS7ZaPk"
      },
      "source": [
        "**Defining Constants**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JkvVddyTZszr"
      },
      "outputs": [],
      "source": [
        "IMAGE_WIDTH = 300\n",
        "IMAGE_HEIGHT = 400\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS = 3\n",
        "data_dir  = \"Building Types\"\n",
        "class_names = ['Bungalow','Highrise','Storey-Building']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aUyU_3bAQ1e"
      },
      "source": [
        "**Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEVaWcw_DI6_"
      },
      "source": [
        "**Importing Important Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ciqWphWvuvqX"
      },
      "outputs": [],
      "source": [
        "#Importing\n",
        "import os\n",
        "\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "import torch.optim as optim\n",
        "\n",
        "import shutil  # For moving files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol9eBbOfn9L4"
      },
      "source": [
        "**Splitting the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN4GZJcDzUsQ",
        "outputId": "28a78af7-6f23-4be6-fd4b-cd29354f6860"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "With n_samples=1, test_size=None and train_size=0.5, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m train_files, temp_files \u001b[38;5;241m=\u001b[39m train_test_split(image_files, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# 80% train\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Split the temp files into val and test\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m val_files, test_files \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 50% of temp = 10% val, 10% test\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Move files to respective directories\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m files, dest_dir \u001b[38;5;129;01min\u001b[39;00m [(train_files, train_dir), (val_files, val_dir), (test_files, test_dir)]:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2785\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2415\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2412\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2419\u001b[0m     )\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
            "\u001b[1;31mValueError\u001b[0m: With n_samples=1, test_size=None and train_size=0.5, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "train_dir = 'train'\n",
        "val_dir = 'val'\n",
        "test_dir = 'test'\n",
        "\n",
        "# Create the destination directories if they don't exist\n",
        "for dir_name in [train_dir, val_dir, test_dir]:\n",
        "    for class_name in os.listdir(data_dir): #Iterate through each class folder\n",
        "        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
        "\n",
        "# Split the data\n",
        "for class_name in os.listdir(data_dir):\n",
        "    class_dir = os.path.join(data_dir, class_name)\n",
        "    image_files = os.listdir(class_dir)\n",
        "\n",
        "    # Split into train and (val+test)\n",
        "    train_files, temp_files = train_test_split(image_files, train_size=0.8, random_state=42)  # 80% train\n",
        "\n",
        "    # Split the temp files into val and test\n",
        "    val_files, test_files = train_test_split(temp_files, train_size=0.5, random_state=42)  # 50% of temp = 10% val, 10% test\n",
        "\n",
        "    # Move files to respective directories\n",
        "    for files, dest_dir in [(train_files, train_dir), (val_files, val_dir), (test_files, test_dir)]:\n",
        "        for file_name in files:\n",
        "            source_path = os.path.join(class_dir, file_name)\n",
        "            destination_path = os.path.join(dest_dir, class_name, file_name) #Corrected the path\n",
        "            shutil.move(source_path, destination_path)\n",
        "\n",
        "print(\"Data split and organized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRYUJzrin7ys"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),  # Resize and crop for data augmentation\n",
        "        transforms.RandomHorizontalFlip(),   # Randomly flip images horizontally\n",
        "        transforms.ToTensor(),              # Convert to PyTorch tensor\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJF2lzXPc2ZW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rPAyZ7vx1z6"
      },
      "outputs": [],
      "source": [
        "# Load datasets using ImageFolder\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(eval(x + '_dir')), data_transforms[x]) for x in ['train', 'val']}\n",
        "\n",
        "# Create data loaders\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes  # Get class names (building types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "zqDvG_Gux1lI",
        "outputId": "95dc6b09-befb-44b0-d42e-b6d13472cefc"
      },
      "outputs": [],
      "source": [
        "plt.imshow(image_datasets['train'][23][0].permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9orh_KxwXBx"
      },
      "outputs": [],
      "source": [
        "# Creating Model\n",
        "\n",
        "# Use a pre-trained model (Transfer Learning)\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify the classifier (last fully connected layer)\n",
        "num_classes = len(class_names)  # Number of building types\n",
        "num_ftrs = model_ft.fc.in_features  # Get input features of the last layer\n",
        "model_ft.fc = nn.Linear(num_ftrs, num_classes)  # Replace with a new linear layer\n",
        "\n",
        "# Move the model to the appropriate device (GPU if available)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft = model_ft.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGUM9IagwW-B",
        "outputId": "d3d128e7-d076-466b-9700-9c75a7beb87c"
      },
      "outputs": [],
      "source": [
        "# 3. Training\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_ft.parameters(), lr=0.001)  # You can use SGD, etc.\n",
        "\n",
        "# Training loop (simplified)\n",
        "num_epochs = 10  # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model_ft.train()  # Set model to training mode\n",
        "        else:\n",
        "            model_ft.eval()   # Set model to evaluation mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "            with torch.set_grad_enabled(phase == 'train'):  # Enable gradients only in training\n",
        "                outputs = model_ft(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()  # Backpropagate the loss\n",
        "                    optimizer.step()  # Update the weights\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg3gM9NhwW2u",
        "outputId": "b81297eb-f395-48fd-be36-d8b8ef8abc43"
      },
      "outputs": [],
      "source": [
        "# Evaluation (on the test set)\n",
        "model_ft.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in dataloaders['val']:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_ft(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diIq9p9kwWyZ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"./model/model_ft.pkl\", 'wb') as model_file:\n",
        "    pickle.dump(model_ft, model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87aXH4MIwWto"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEjdTSeBwWqW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su_kN7Rzuvkt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aCwI4Pouvhp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ebSlS-uuvfC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL5UNkXpuvcw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dgu-_PiuvaP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
